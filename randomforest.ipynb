{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "yukzGk3jUroY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lv3yM1CJwESf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer #, KNNImputer, IterativeImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Load the dataset\n",
        "# Assuming the dataset is a CSV file in Google Drive\n",
        "data = pd.read_csv('/content/drive/My Drive/mayhoc/train.csv')"
      ],
      "metadata": {
        "id": "25KKdVCOwT8l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique classes\n",
        "unique_classes = data['Label'].unique()\n",
        "\n",
        "# Count the number of classes\n",
        "num_classes = len(unique_classes)\n",
        "# Count the number of columns\n",
        "num_columns = data.shape[1]\n",
        "\n",
        "# Data distribution for the \"Label\" column\n",
        "label_distribution = data['Label'].value_counts()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of columns: {num_columns}\")\n",
        "print(\"\\nData Distribution by 'Label':\")\n",
        "print(label_distribution)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of unique classes: {num_classes}\")\n",
        "print(\"Classes:\", unique_classes)"
      ],
      "metadata": {
        "id": "REMGsYHGwmK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace inf values\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Explicitly name the target column\n",
        "target_column = 'Label'\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "data[target_column] = label_encoder.fit_transform(data[target_column])\n",
        "\n",
        "# Separating out the features and target\n",
        "features = data.drop(columns=[target_column, 'ID', 'Weight']).columns  # Exclude \"Weight\" from features\n",
        "X = data.loc[:, features].values\n",
        "y = data.loc[:, target_column].values\n",
        "weights = data[\"Weight\"].values  # Get the weights\n",
        "\n",
        "#fill nan values using SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imputed_x = imputer.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets, including weights\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
        "    X, y, weights, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Q6_farStwy14"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimize hyperparamater using optuna\n",
        "import optuna\n",
        "## Define objective function\n",
        "def objective(trial):\n",
        "    # Suggest values for hyperparameters\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
        "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "\n",
        "    # Create and fit random forest model\n",
        "    model = RandomForestRegressor(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    min_samples_split=min_samples_split,\n",
        "    min_samples_leaf=min_samples_leaf,\n",
        "    random_state=42,\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate RMSE\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return MAE\n",
        "    return mae\n",
        "\n",
        "# Create study object\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Run optimization process\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(\"Best trial:\", study.best_trial)\n",
        "print(\"Best hyperparameters:\", study.best_params)"
      ],
      "metadata": {
        "id": "BKiAqFasUKTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42,n_jobs=-1,verbose=2)\n",
        "\n",
        "# 2. Train the model with sample weights\n",
        "rf_classifier.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# 3. Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# 4. Calculate weighted accuracy and F1-score\n",
        "weighted_accuracy = accuracy_score(y_test, y_pred, sample_weight=weights_test)\n",
        "weighted_f1 = f1_score(y_test, y_pred, average=\"weighted\", sample_weight=weights_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(f\"Weighted Accuracy: {weighted_accuracy:.4f}\")\n",
        "print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "2nzra1BixK10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance Features witch Scikit Learn\n",
        "# Built-in feature importance (Gini Importance)\n",
        "importances = rf_classifier.feature_importances_\n",
        "feature_imp_df = pd.DataFrame({'Feature': features, 'Gini Importance': importances}).sort_values('Gini Importance', ascending=False)\n",
        "print(feature_imp_df)\n",
        "\n",
        "# Create a bar plot for feature importance\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(features, importances, color='skyblue')\n",
        "plt.xlabel('Gini Importance')\n",
        "plt.title('Feature Importance - Gini Importance')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "boqbAr5RT1BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export model using pickle\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# save the iris classification model as a pickle file\n",
        "model_pkl_file = \"ddos-detection.pkl\"\n",
        "\n",
        "# with open(model_pkl_file, 'wb') as file:\n",
        "with gzip.open(model_pkl_file, 'wb') as file:\n",
        "    pickle.dump(rf_classifier, file)"
      ],
      "metadata": {
        "id": "U2kA0oTrVDev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}